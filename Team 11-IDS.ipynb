{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef13efe",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš¨ Intrusion Detection System (IDS) with XGBoost â€” NSL-KDD\n",
    "\n",
    "**Team 11: AI for Cybersecurity** â€” *Model Engineer Notebook*\n",
    "\n",
    "This notebook builds an Intrusion Detection System (IDS) using the **NSL-KDD** dataset and the **XGBoost** algorithm, following an industry-standard ML workflow.\n",
    "\n",
    "## Objectives\n",
    "- Load and explore the NSL-KDD dataset\n",
    "- Preprocess data (encoding, scaling, train/test split, SMOTE)\n",
    "- Train baseline and XGBoost models\n",
    "- Evaluate with classification metrics & confusion matrix\n",
    "- Explain predictions with **SHAP**\n",
    "- Discuss ethics and limitations\n",
    "- Export the trained model for reuse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0a968",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "> Run this cell to install required libraries if missing (e.g., in Colab). If you already have them, you can skip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment (e.g., Colab), uncomment to install:\n",
    "# %pip install -q xgboost scikit-learn pandas numpy shap imbalanced-learn kagglehub matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215607e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             f1_score, precision_score, recall_score, accuracy_score)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['axes.grid'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf14643",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Exploration\n",
    "\n",
    "This section attempts to locate NSL-KDD files automatically. **You have two options**:\n",
    "\n",
    "- **Option A (Recommended):** Use `kagglehub` to download the dataset programmatically.\n",
    "- **Option B:** Manually place the files locally (e.g., `./data/`) and set `DATA_DIR` accordingly.\n",
    "\n",
    "**Common NSL-KDD file names:**\n",
    "- `KDDTrain+.csv` or `KDDTrain+.txt`\n",
    "- `KDDTest+.csv` or `KDDTest+.txt`\n",
    "\n",
    "> The notebook is robust to either `.csv` or `.txt` delimiters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d663a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Locate or download dataset\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_candidates = [\n",
    "    DATA_DIR / \"KDDTrain+.csv\", DATA_DIR / \"KDDTrain+.txt\",\n",
    "    DATA_DIR / \"KDDTrain.csv\",  DATA_DIR / \"KDDTrain.txt\"\n",
    "]\n",
    "test_candidates = [\n",
    "    DATA_DIR / \"KDDTest+.csv\", DATA_DIR / \"KDDTest+.txt\",\n",
    "    DATA_DIR / \"KDDTest.csv\",  DATA_DIR / \"KDDTest.txt\"\n",
    "]\n",
    "\n",
    "# Try kagglehub if files missing\n",
    "if not any(p.exists() for p in train_candidates+test_candidates):\n",
    "    try:\n",
    "        import kagglehub\n",
    "        path = Path(kagglehub.dataset_download(\"hassan06/nslkdd\"))\n",
    "        # Move files into DATA_DIR if needed\n",
    "        for p in path.iterdir():\n",
    "            if p.is_file():\n",
    "                dest = DATA_DIR / p.name\n",
    "                if not dest.exists():\n",
    "                    dest.write_bytes(p.read_bytes())\n",
    "        print(\"Downloaded with kagglehub to:\", DATA_DIR.resolve())\n",
    "    except Exception as e:\n",
    "        print(\"kagglehub download failed or not available:\", e)\n",
    "        print(\"Please place NSL-KDD files into\", DATA_DIR.resolve())\n",
    "\n",
    "def load_nsl_kdd(train_path, test_path):\n",
    "    # Try to infer delimiter\n",
    "    def read_any(p):\n",
    "        p = Path(p)\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            return pd.read_csv(p)\n",
    "        elif p.suffix.lower() == \".txt\":\n",
    "            # NSL-KDD txts are comma-separated\n",
    "            return pd.read_csv(p, header=None)\n",
    "        else:\n",
    "            # Fallback\n",
    "            return pd.read_csv(p)\n",
    "    df_train = read_any(train_path)\n",
    "    df_test  = read_any(test_path)\n",
    "    return df_train, df_test\n",
    "\n",
    "# Column names (NSL-KDD has 41 feature columns + 'label' + 'difficulty')\n",
    "NSL_KDD_COLUMNS = [\n",
    "    \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
    "    \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
    "    \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\n",
    "    \"label\",\"difficulty\"\n",
    "]\n",
    "\n",
    "# Find actual files\n",
    "def find_first_existing(cands):\n",
    "    for p in cands:\n",
    "        if Path(p).exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "train_file = find_first_existing(train_candidates)\n",
    "test_file  = find_first_existing(test_candidates)\n",
    "\n",
    "if train_file is None or test_file is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"NSL-KDD files not found in {DATA_DIR.resolve()}. \"\n",
    "        \"Expected something like KDDTrain+.csv/.txt and KDDTest+.csv/.txt.\"\n",
    "    )\n",
    "\n",
    "df_train, df_test = load_nsl_kdd(train_file, test_file)\n",
    "\n",
    "# If files had no headers, apply column names\n",
    "if df_train.shape[1] == len(NSL_KDD_COLUMNS):\n",
    "    df_train.columns = NSL_KDD_COLUMNS\n",
    "if df_test.shape[1] == len(NSL_KDD_COLUMNS):\n",
    "    df_test.columns = NSL_KDD_COLUMNS\n",
    "\n",
    "print(\"Train shape:\", df_train.shape, \"| Test shape:\", df_test.shape)\n",
    "display(df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f5861",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Basic EDA\n",
    "- Shapes, dtypes, missing values\n",
    "- Class distribution (multi-class & binary)\n",
    "- Quick feature correlation (numeric only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dtypes & missing\n",
    "display(df_train.dtypes.head(20))\n",
    "print(\"\\nMissing values (train):\\n\", df_train.isna().sum().sum())\n",
    "print(\"Missing values (test):\", df_test.isna().sum().sum())\n",
    "\n",
    "# Class distribution (multi-class)\n",
    "print(\"\\nLabel distribution (train):\\n\", df_train['label'].value_counts())\n",
    "\n",
    "# Binary mapping for overview\n",
    "def to_binary(y):\n",
    "    return (y != 'normal').astype(int)\n",
    "\n",
    "y_train_bin = to_binary(df_train['label'])\n",
    "print(\"\\nBinary distribution (train):\\n\", pd.Series(y_train_bin).value_counts())\n",
    "\n",
    "# Quick numeric correlation (sample to speed up)\n",
    "num_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(num_cols) > 0:\n",
    "    corr = df_train[num_cols].sample(min(len(df_train), 5000), random_state=42).corr()\n",
    "    plt.figure()\n",
    "    plt.imshow(corr, aspect='auto')\n",
    "    plt.title(\"Numeric Feature Correlation (sample)\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c3312",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "- Handle missing values (imputation)\n",
    "- Encode categoricals (`protocol_type`, `service`, `flag`)\n",
    "- Scale numericals\n",
    "- Convert labels to binary (normal â†’ 0, attacks â†’ 1)\n",
    "- **Important:** Apply **SMOTE on the training set only** to avoid leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features / labels\n",
    "X_train_raw = df_train.drop(columns=['label', 'difficulty'], errors='ignore')\n",
    "X_test_raw  = df_test.drop(columns=['label', 'difficulty'], errors='ignore')\n",
    "y_train_mc  = df_train['label'].copy()\n",
    "y_test_mc   = df_test['label'].copy()\n",
    "\n",
    "# Binary labels\n",
    "y_train = (y_train_mc != 'normal').astype(int)\n",
    "y_test  = (y_test_mc  != 'normal').astype(int)\n",
    "\n",
    "# Identify categorical / numerical columns\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "numerical_features = [c for c in X_train_raw.columns if c not in categorical_features]\n",
    "\n",
    "# Preprocessors\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train/test split is already provided by NSL-KDD (KDDTrain+ vs KDDTest+)\n",
    "# We will still reserve a validation split from the train set for tuning if needed.\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_raw, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Train split:\", X_tr.shape, \"Val split:\", X_val.shape)\n",
    "\n",
    "# SMOTE only on the training portion\n",
    "smote = SMOTE(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd602c68",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Baseline Model (Random Forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_clf = RandomForestClassifier(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_pipe = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"smote\", smote),\n",
    "    (\"clf\", baseline_clf)\n",
    "])\n",
    "\n",
    "baseline_pipe.fit(X_tr, y_tr)\n",
    "\n",
    "def evaluate_model(estimator, X, y, label=\"Eval\"):\n",
    "    y_pred = estimator.predict(X)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = estimator.predict_proba(X)[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    print(f\"=== {label} ===\")\n",
    "    print(f\"F1: {f1:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | Accuracy: {acc:.4f} | FPR: {fpr:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n",
    "    # Confusion matrix plot\n",
    "    cm = np.array([[tn, fp],[fn, tp]])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, cmap=None)\n",
    "    plt.title(f\"Confusion Matrix â€” {label}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, int(v), ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return {\"f1\": f1, \"precision\": prec, \"recall\": rec, \"accuracy\": acc, \"fpr\": fpr}\n",
    "\n",
    "print(\"\\nBaseline on Validation Set:\")\n",
    "baseline_val_metrics = evaluate_model(baseline_pipe, X_val, y_val, label=\"Baseline (Val)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fa353",
   "metadata": {},
   "source": [
    "\n",
    "## 6. XGBoost Model â€” Training & Hyperparameter Tuning\n",
    "We use the **scikit-learn wrapper** for smooth integration with pipelines and GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\"  # fast & memory efficient\n",
    ")\n",
    "\n",
    "xgb_pipe = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"smote\", smote),\n",
    "    (\"clf\", xgb)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__max_depth\": [4, 6, 8],\n",
    "    \"clf__learning_rate\": [0.05, 0.1],\n",
    "    \"clf__subsample\": [0.8, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    xgb_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_tr, y_tr)\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "print(\"\\nValidation Performance (Best XGB):\")\n",
    "xgb_val_metrics = evaluate_model(best_xgb, X_val, y_val, label=\"XGB (Val)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826252cf",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e414272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Baseline on Test Set:\")\n",
    "_ = evaluate_model(baseline_pipe, X_test_raw, y_test, label=\"Baseline (Test)\")\n",
    "\n",
    "print(\"\\nXGB on Test Set:\")\n",
    "_ = evaluate_model(best_xgb, X_test_raw, y_test, label=\"XGB (Test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c070e7",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Model Interpretability with SHAP\n",
    "We compute **SHAP values** for the best XGBoost model to explain which features drive predictions.\n",
    "> Note: We compute SHAP on a sample for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ffb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract trained XGB model and the fitted preprocessor from the pipeline\n",
    "fitted_preprocessor = best_xgb.named_steps[\"preprocess\"]\n",
    "fitted_clf = best_xgb.named_steps[\"clf\"]\n",
    "\n",
    "# Transform a sample of validation data to model-ready numeric features\n",
    "sample_idx = np.random.RandomState(42).choice(len(X_val), size=min(2000, len(X_val)), replace=False)\n",
    "X_val_sample = X_val.iloc[sample_idx]\n",
    "y_val_sample = y_val.iloc[sample_idx]\n",
    "\n",
    "X_val_transformed = fitted_preprocessor.transform(X_val_sample)\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "def get_feature_names(preprocessor, numeric_features, categorical_features):\n",
    "    num_features_out = numeric_features\n",
    "    cat_encoder = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_features_out = cat_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "    return num_features_out + cat_features_out\n",
    "\n",
    "feature_names = get_feature_names(fitted_preprocessor, \n",
    "                                  fitted_preprocessor.transformers_[0][2], \n",
    "                                  fitted_preprocessor.transformers_[1][2])\n",
    "\n",
    "# SHAP analysis\n",
    "explainer = shap.TreeExplainer(fitted_clf)\n",
    "shap_values = explainer.shap_values(X_val_transformed)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_val_transformed, feature_names=feature_names, show=True)\n",
    "plt.show()\n",
    "\n",
    "# Dependence plot for a top feature\n",
    "top_feature_idx = np.argsort(np.abs(shap_values).mean(axis=0))[-1]\n",
    "top_feature_name = feature_names[top_feature_idx]\n",
    "shap.dependence_plot(top_feature_name, shap_values, X_val_transformed, feature_names=feature_names, show=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b29569",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Ethical Analysis\n",
    "\n",
    "- **Privacy:** Even anonymized network telemetry can reveal sensitive behavior. Use data minimization and access controls; avoid storing PII; follow GDPR-like principles where applicable.\n",
    "- **False Positives:** High FPR can overwhelm analysts and cause alert fatigue. Balance precision/recall carefully; route low-confidence alerts to sandbox rather than block.\n",
    "- **Bias & Coverage:** Rare attack types may be under-detected. Use techniques like **SMOTE**, targeted data augmentation, and periodic re-training on fresh telemetry.\n",
    "- **Human-in-the-Loop:** Keep humans in decision loops for critical actions (blocking/quarantine). Provide explanations (e.g., **SHAP**) to support trust and oversight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c255a8",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Conclusion & Future Work\n",
    "\n",
    "**Summary:** We built an IDS on NSL-KDD with an XGBoost core, addressed class imbalance via SMOTE, and explained predictions using SHAP. We benchmarked against a Random Forest baseline and evaluated with F1, precision, recall, accuracy, and FPR.\n",
    "\n",
    "**Future Enhancements:**\n",
    "- Add MLflow for experiment tracking and reproducibility\n",
    "- Evaluate deep learning or autoencoder-based anomaly detection\n",
    "- Explore streaming inference and real-time features\n",
    "- Perform feature drift and data quality monitoring in production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbc79b",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Export Trained Model\n",
    "Save the trained XGBoost pipeline to disk for reuse in downstream applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_DIR = Path(\"./models\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = OUTPUT_DIR / \"xgb_nslkdd_pipeline.joblib\"\n",
    "joblib.dump(best_xgb, model_path)\n",
    "print(\"Saved model to:\", model_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f6b53",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Appendix: Model Spec & Config\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  name: xgboost_classifier\n",
    "  objective: binary:logistic\n",
    "  metrics: [f1, precision, recall, accuracy, fpr]\n",
    "  imbalance: SMOTE\n",
    "  interpretability: SHAP\n",
    "  selection: grid_search (cv=3)\n",
    "  features:\n",
    "    categorical: [protocol_type, service, flag]\n",
    "    numerical: all_others\n",
    "artifacts:\n",
    "  pipeline: models/xgb_nslkdd_pipeline.joblib\n",
    "  notebooks: this_notebook.ipynb\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
